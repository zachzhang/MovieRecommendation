{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class HybridCollabFilter():\n",
    "\n",
    "    def __init__(self, numUsers, embedding_dim_plot, embedding_dim, input_dim):\n",
    "\n",
    "        # hyper parameters\n",
    "        self.batch_size = 300\n",
    "        self.numUsers = numUsers\n",
    "        self.epochs = 10\n",
    "        self.init_var =.01\n",
    "\n",
    "        #Movie Features\n",
    "        self.movieFeatures = tf.placeholder(tf.float32, shape=(None,input_dim))\n",
    "\n",
    "        # input tensors for movies, usres, ratings\n",
    "        self.users = tf.placeholder(tf.int32, shape=(None))\n",
    "        self.rating = tf.placeholder(tf.float32, shape=(None))\n",
    "\n",
    "        # embedding matricies for users\n",
    "        self.userMat = tf.Variable(self.init_var*tf.random_normal([numUsers, embedding_dim_plot]))\n",
    "        self.userBias = tf.Variable(self.init_var*tf.random_normal([numUsers,]))\n",
    "\n",
    "        #Model parameters for movies\n",
    "        self.W = tf.Variable(self.init_var*tf.random_normal([input_dim, embedding_dim_plot]))\n",
    "        self.b = tf.Variable(self.init_var*tf.random_normal([embedding_dim_plot]))\n",
    "\n",
    "        movieTensor = tf.matmul(self.movieFeatures,self.W) + self.b\n",
    "\n",
    "        # map each user/movie to its feature vector\n",
    "        self.U = tf.nn.embedding_lookup(self.userMat, self.users)\n",
    "        self.u_b = tf.nn.embedding_lookup(self.userBias, self.users)\n",
    "\n",
    "        # predicted rating is dot product of user and movie\n",
    "        self.yhat = tf.reduce_sum(tf.mul(self.U, movieTensor) , 1) + self.u_b\n",
    "\n",
    "        self.cost = tf.nn.l2_loss(self.yhat - self.rating)\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=.01).minimize(self.cost)\n",
    "        \n",
    "        self.session = tf.Session()\n",
    "        self.session.run(tf.initialize_all_variables())\n",
    "\n",
    "\n",
    "    def train_test_split(self,users,movies,ratings,split=.1):\n",
    "\n",
    "        shuffle  = np.random.permutation(len(users))\n",
    "\n",
    "        partition = np.floor(len(users) * (1-split))\n",
    "\n",
    "        train_idx = shuffle[:partition]\n",
    "        test_idx = shuffle[partition:]\n",
    "\n",
    "        users_train = users[train_idx]\n",
    "        users_test = users[test_idx]\n",
    "\n",
    "        movies_train = movies[train_idx]\n",
    "        movies_test = movies[test_idx]\n",
    "\n",
    "        ratings_train = ratings[train_idx]\n",
    "        ratings_test = ratings[test_idx]\n",
    "\n",
    "        return users_train,movies_train,ratings_train , users_test,movies_test,ratings_test\n",
    "\n",
    "\n",
    "    def train(self, users, movies, ratings,val_freq=5):\n",
    "\n",
    "        users_train, movies_train, ratings_train, users_test, movies_test, ratings_test = \\\n",
    "            self.train_test_split(users,movies,ratings)\n",
    "\n",
    "        num_batches = movies_train.shape[0] // self.batch_size\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            avg_cost = 0\n",
    "\n",
    "            for b_idx in range(num_batches):\n",
    "\n",
    "                ratings_batch  = ratings_train[self.batch_size * b_idx:self.batch_size * (b_idx + 1)]\n",
    "\n",
    "                users_batch = users_train[self.batch_size * b_idx:self.batch_size * (b_idx + 1)]\n",
    "                movie_batch = movies_train[self.batch_size * b_idx:self.batch_size * (b_idx + 1)]\n",
    "\n",
    "                avg_cost +=  (self.session.run([self.cost, self.optimizer],\n",
    "                                             {self.users: users_batch, self.movieFeatures: movie_batch,\n",
    "                                              self.rating: ratings_batch})[0] ) / self.batch_size\n",
    "\n",
    "\n",
    "\n",
    "            print (\"Epoch: \", i, \" Average Cost: \",avg_cost / num_batches)\n",
    "\n",
    "            if i % val_freq ==0:\n",
    "                auc_mean = 0\n",
    "                uni_users = np.unique(users_test)\n",
    "                for usr in uni_users:\n",
    "                    usr_idxes = users_test == usr\n",
    "                    usr_idxes = np.where(usr_idxes)\n",
    "                    usr_u = users_test[usr_idxes]\n",
    "                    movie_u = movies_test[usr_idxes]\n",
    "                    rtg_u = ratings_test[usr_idxes]\n",
    "                    if len(usr_u) < 3:\n",
    "                        continue\n",
    "                    yhat = (self.session.run([self.yhat],\n",
    "                                             {self.users: usr_u, self.movieFeatures: movie_u,\n",
    "                                              self.rating: rtg_u})[0] )\n",
    "                    auc_mean += sklearn.metrics.auc(yhat, rtg_u, reorder = True) / len(uni_users)\n",
    "\n",
    "                print (\"Testing AUC mean: \" , auc_mean)\n",
    "                \n",
    "\n",
    "    @staticmethod\n",
    "    def map2idx(movieratings, mergedScrape_ML):\n",
    "\n",
    "        users = movieratings['userId'].values\n",
    "        movies = movieratings['movieId'].values\n",
    "\n",
    "        # unique users / movies\n",
    "        uni_users = movieratings['userId'].unique()\n",
    "        uni_movies = mergedScrape_ML['movieId'].unique()\n",
    "\n",
    "        print len(uni_movies)\n",
    "\n",
    "        # dict mapping the id to an index\n",
    "        user_map = dict(zip(uni_users, range(len(uni_users))))\n",
    "        movie_map = dict(zip(uni_movies, range(len(uni_movies))))\n",
    "\n",
    "        pairs = []\n",
    "        for user, movie, rating in zip(users, movies, movieratings['rating']):\n",
    "            if movie in movie_map:\n",
    "                pairs.append((user_map[user], movie_map[movie], rating))\n",
    "\n",
    "        return np.array(pairs), len(uni_users), len(uni_movies)\n",
    "\n",
    "\n",
    "def featureMatrix(movieData):\n",
    "    \n",
    "    #movieData.ix[pd.isnull(movieData[['plot']]).as_matrix()[:, 0], 'plot'] = ''\n",
    "\n",
    "    movieData[pd.isnull(movieData)] = ''\n",
    "    \n",
    "    vectorizer = CountVectorizer(max_features=200)\n",
    "\n",
    "    vectorizer.fit(movieData['plot'])\n",
    "\n",
    "    movieFeaturesplot = vectorizer.transform(movieData['plot'])\n",
    "\n",
    "    \n",
    "    \n",
    "    return movieFeatures.toarray()\n",
    "\n",
    "\n",
    "#The first iteration here will be just using plot\n",
    "if __name__ == '__main__':\n",
    "    scrapedMovieData = pd.read_csv('movieDataList.csv', index_col=0)\n",
    "\n",
    "    # Movie Lens rating data\n",
    "    movieratings = pd.read_csv('ratings.csv')\n",
    "\n",
    "    # List of movies in order\n",
    "    movieLenseMovies = pd.read_csv('movies.csv')\n",
    "\n",
    "    featMat = featureMatrix(scrapedMovieData)\n",
    "\n",
    "    movieLenseMovies.drop('genres', axis=1, inplace=True)\n",
    "\n",
    "    mergedScrape_ML = pd.merge(scrapedMovieData, movieLenseMovies, left_on='movie_len_title',\n",
    "                               right_on='title',\n",
    "                               how='left')\n",
    "\n",
    "    mergedScrape_ML.drop_duplicates(subset='movie_len_title', inplace=True)\n",
    "\n",
    "    #User and movie ids mapped to be on continuous interval\n",
    "    triples, num_users, num_movie = HybridCollabFilter.map2idx(movieratings,mergedScrape_ML)\n",
    "\n",
    "    user_idx = triples[:,0]\n",
    "    movie_idx = triples[ :,1]\n",
    "    ratings = triples[:, 2]\n",
    "\n",
    "    movieFeatures = featMat[movie_idx.astype(int)]\n",
    "\n",
    "\n",
    "    #(self, numUsers, embedding_dim_plot, embedding_dim,input_dim):\n",
    "    movieModel = HybridCollabFilter(num_users, 5, 5, 200)\n",
    "    movieModel.train(user_idx, movieFeatures, ratings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scrapedMovieData = pd.read_csv('movieDataList.csv', index_col=0).fillna('')\n",
    "\n",
    "movieData = scrapedMovieData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating                  0\n",
       "runtimes                0\n",
       "year                    0\n",
       "languages               0\n",
       "votes                   0\n",
       "producer                0\n",
       "title                   0\n",
       "writer                  0\n",
       "editor                  0\n",
       "certificates            0\n",
       "country codes           0\n",
       "language codes          0\n",
       "cover url               0\n",
       "genres                  0\n",
       "director                0\n",
       "production companies    0\n",
       "countries               0\n",
       "plot outline            0\n",
       "plot                    0\n",
       "cast                    0\n",
       "original music          0\n",
       "full-size cover url     0\n",
       "movie_len_title         0\n",
       "mpaa                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isnull(movieData.fillna('')).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cast = movieData['cast']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_cast = cast[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hanks_Tom_', 'Allen_Tim_', 'Rickles_Don_', 'Varney_Jim_', 'Shawn_Wallace_', 'Ratzenberger_John_', 'Potts_Annie_', 'Morris_John_', 'vonDetten_Erik_', 'Metcalf_Laurie_', 'Ermey_R.Lee_', 'Freeman_Sarah_', 'Jillette_Penn_', 'Angel_Jack_', 'Aste_Spencer_', 'Berg_Greg_', 'Bradley_Lisa_', 'Cunningham_Kendall_', 'Derryberry_Debi_', 'Dorkin_Cody_', 'Farmer_Bill_', 'Good_Craig_', 'Grudt_Gregory_', 'Judovits_Danielle_', 'Lasseter_Sam_', 'Levenbrown_Brittany_', 'Lynn_Sherry_', 'McAfee_Scott_', 'McGowan_Mickie_', \"O'Donohue_Ryan_\", 'Pidgeon_Jeff_', 'Pinney_Patrick_', 'Proctor_Phil_', 'Rabson_Jan_', 'Ranft_Joe_', 'Stanton_Andrew_', 'Sweet_Shane_', 'Erbil_MehmetAli_', 'Lane_Nathan_', 'Lasseter_John_', 'Sabella_Ernie_', 'Unkrich_Hannah_', 'Welker_Frank_']\n"
     ]
    }
   ],
   "source": [
    "test_cast\n",
    "\n",
    "c = test_cast.split('>')\n",
    "c2 = [ s.split(\":_\")[1] for s in c[0:-1]]\n",
    "c3 = [ \"_\".join(s.split(\",\")).replace(\" \", \"\") for s in c2]\n",
    "\n",
    "print c3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_person_string(raw_text):\n",
    "\n",
    "    if raw_text == '':\n",
    "        return ''\n",
    "\n",
    "    cast = raw_text.split('>')\n",
    "    cast = [ s.split(\":_\")[1] for s in cast[0:-1]]\n",
    "    cast = [\"_\".join(s.split(\",\")).replace(\" \", \"\") for s in cast]\n",
    "\n",
    "    return \" \".join(cast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cast_str = movieData['cast'].apply(clean_person_string)\n",
    "director_str = movieData['director'].apply(clean_person_string)\n",
    "\n",
    "people_df = pd.DataFrame([cast_str,director_str])\n",
    "\n",
    "people_strings = people_df.apply( lambda x:  ' '.join(x) , axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hanks_Tom_ Allen_Tim_ Rickles_Don_ Varney_Jim_ Shawn_Wallace_ Ratzenberger_John_ Potts_Annie_ Morris_John_ vonDetten_Erik_ Metcalf_Laurie_ Ermey_R.Lee_ Freeman_Sarah_ Jillette_Penn_ Angel_Jack_ Aste_Spencer_ Berg_Greg_ Bradley_Lisa_ Cunningham_Kendall_ Derryberry_Debi_ Dorkin_Cody_ Farmer_Bill_ Good_Craig_ Grudt_Gregory_ Judovits_Danielle_ Lasseter_Sam_ Levenbrown_Brittany_ Lynn_Sherry_ McAfee_Scott_ McGowan_Mickie_ O'Donohue_Ryan_ Pidgeon_Jeff_ Pinney_Patrick_ Proctor_Phil_ Rabson_Jan_ Ranft_Joe_ Stanton_Andrew_ Sweet_Shane_ Erbil_MehmetAli_ Lane_Nathan_ Lasseter_John_ Sabella_Ernie_ Unkrich_Hannah_ Welker_Frank_ Lasseter_John_\""
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_strings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lasseter_John_'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "director_str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
